services:
  # PostgreSQL with TimescaleDB and PostGIS for time-series and spatial data storage
  timescaledb:
    image: timescale/timescaledb-ha:pg15-latest
    container_name: aqi_timescaledb
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-aqi_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-aqi_password}
      POSTGRES_DB: ${POSTGRES_DB:-aqi_db}
    # Port not exposed externally - only accessible via internal Docker network
    # ports:
    #   - "5433:5432"
    volumes:
      - timescale_data:/var/lib/postgresql/data
      - ./database/init:/docker-entrypoint-initdb.d
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-aqi_user} -d ${POSTGRES_DB:-aqi_db}" ]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - aqi_network

  # API service for data access and model serving
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: aqi_api
    restart: unless-stopped
    environment:
      - DATABASE_URL=postgresql://${POSTGRES_USER:-aqi_user}:${POSTGRES_PASSWORD:-aqi_password}@timescaledb:5432/${POSTGRES_DB:-aqi_db}
      - ENVIRONMENT=${ENVIRONMENT:-development}
      - OLLAMA_MODEL=qwen2.5:1.5b
      - OLLAMA_TIMEOUT=60.0
      # Anthropic Claude AI configuration (for performance comparison)
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - CLAUDE_MODEL=${CLAUDE_MODEL:-claude-3-haiku-20240307}
      - CLAUDE_TIMEOUT=30.0
    # Port not exposed externally - accessible via Nginx reverse proxy on port 5800
    # ports:
    #   - "8000:8000"
    depends_on:
      timescaledb:
        condition: service_healthy
    volumes:
      - ./backend_model:/app/backend_model
      - ./backend_api:/app/backend_api
      - ./models:/app/models
      - ./logs:/app/logs
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:8000/health || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - aqi_network
    # Scale API for higher load
    # docker compose up --scale api=3
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
        reservations:
          memory: 512M
          cpus: '0.5'

  # Frontend served by Nginx (scalable, cacheable)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: aqi_frontend
    restart: unless-stopped
    ports:
      - "5800:80"
    depends_on:
      api:
        condition: service_healthy
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:80/ebot/ || exit 1" ]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - aqi_network
    # Scale frontend independently
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.5'
        reservations:
          memory: 32M
          cpus: '0.1'

  # Scheduler service for automated data ingestion and imputation
  scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: aqi_scheduler
    restart: unless-stopped
    command: python -m backend_api.scheduler
    environment:
      - DATABASE_URL=postgresql://${POSTGRES_USER:-aqi_user}:${POSTGRES_PASSWORD:-aqi_password}@timescaledb:5432/${POSTGRES_DB:-aqi_db}
      - ENVIRONMENT=production
    depends_on:
      timescaledb:
        condition: service_healthy
    volumes:
      - ./backend_model:/app/backend_model
      - ./backend_api:/app/backend_api
      - ./models:/app/models
      - ./logs:/app/logs
    healthcheck:
      test: [ "CMD-SHELL", "pgrep -f 'backend_api.scheduler' || exit 1" ]
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 120s
    networks:
      - aqi_network
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 256M
          cpus: '0.25'

  # Ollama - Local LLM inference server for AI chatbot
  ollama:
    image: ollama/ollama:latest
    container_name: aqi_ollama
    restart: unless-stopped
    environment:
      - OLLAMA_MODEL=qwen2.5:1.5b
    # Port not exposed externally - only accessible via internal Docker network
    # ports:
    #   - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./ollama-entrypoint.sh:/usr/local/bin/ollama-entrypoint.sh:ro
    entrypoint: [ "/bin/bash", "/usr/local/bin/ollama-entrypoint.sh" ]
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s
    networks:
      - aqi_network
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    # Model is now auto-downloaded on first startup via entrypoint script

volumes:
  timescale_data:
  ollama_data:


networks:
  aqi_network:
    driver: bridge
